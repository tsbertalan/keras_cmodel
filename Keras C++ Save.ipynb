{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(2, activation='tanh'),\n",
    "    tf.keras.layers.Dense(2,  activation='linear'),\n",
    "])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "32/32 [==============================] - 0s 893us/step - loss: 0.6680\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2efc3f9c18>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "model.fit(\n",
    "    np.random.uniform(size=(1000, 2)),\n",
    "    np.random.uniform(size=(1000, 2))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCode:\n",
    "    \n",
    "    def __init__(self, code, header):\n",
    "        self.code = code\n",
    "        self.header = header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CArrayConstant(CCode):\n",
    "    def __init__(self, name, a, const=False, declared=True):\n",
    "        if not isinstance(a, np.ndarray):\n",
    "            assert isinstance(a, tuple)\n",
    "            a = np.zeros(a)\n",
    "        assert len(a.shape) == 2, 'We assume 2D arrays.'\n",
    "        shape_name = name + '_shape'\n",
    "        shape_declaration = 'const int {name}[{size}]'.format(\n",
    "            name=shape_name,\n",
    "            size=len(a.shape),\n",
    "        )\n",
    "        \n",
    "        shape = '{name} = {{{data}}}'.format(\n",
    "            name=shape_name,\n",
    "            data=', '.join(['%d' % s for s in a.shape]),\n",
    "        )\n",
    "        \n",
    "        data_name = name\n",
    "        data_declaration = '{const}{dtypename} {name}[{size}]'.format(\n",
    "            const='const ' if const else '',\n",
    "            dtypename='int' if a.dtype == 'int' else 'double',\n",
    "            size=a.size,\n",
    "            name=data_name,\n",
    "        )\n",
    "        data = '{name} = {{{data}}}'.format(\n",
    "            name=data_name,\n",
    "            data=', '.join(['%.60g' % f for f in a.ravel()]),\n",
    "        )\n",
    "        self.code = shape + ';\\n' + data + ';\\n'\n",
    "        self.actual_header = shape_declaration + ';\\n' + data_declaration + ';\\n'\n",
    "        self.header = self.actual_header if declared else ''\n",
    "        self.name = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs.append(\n",
    "    CArrayConstant(\n",
    "        'input', \n",
    "        (batch_size, int(model.input.shape[1])),\n",
    "        const=False,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "statements = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(CCode):\n",
    "\n",
    "    def __init__(self, i, activation):\n",
    "        actstr = str(activation).lower()\n",
    "        relu = 'relu' in actstr\n",
    "        tanh = 'tanh' in actstr\n",
    "        linear = (not relu) and (not tanh)\n",
    "        self.code = '''\n",
    "        mmult(\n",
    "            preactivation_{i}, preactivation_{i}_shape[0], preactivation_{i}_shape[1],\n",
    "            kernel_{i}, kernel_{i}_shape[0], kernel_{i}_shape[1],\n",
    "            xA_{i}\n",
    "            );\n",
    "        bias_add(\n",
    "            xA_{i}, xA_{i}_shape[0], xA_{i}_shape[1],\n",
    "            bias_{i},\n",
    "            preactivation_{iPlusOne}\n",
    "        );\n",
    "        activate(\n",
    "            preactivation_{iPlusOne}, \n",
    "            preactivation_{iPlusOne}_shape[0],\n",
    "            preactivation_{iPlusOne}_shape[1],\n",
    "            {act_type}\n",
    "        );\n",
    "        '''.format(\n",
    "            i=i, iPlusOne=i+1,\n",
    "            act_type=(0 if relu else (1 if tanh else 2)),\n",
    "        )\n",
    "        self.header = ''\n",
    "        self.name = 'layer_%d' % i\n",
    "        self.i = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer in enumerate(model.layers):\n",
    "    kernel, bias = [x.numpy() for x in layer.weights]\n",
    "    pairs.append(CArrayConstant('kernel_%d' % i, kernel))\n",
    "    act_shape = (batch_size, kernel.shape[1])\n",
    "    pairs.append(CArrayConstant('xA_%d'  % i, act_shape, const=False))\n",
    "    last_layer = i == len(model.layers) - 1\n",
    "    pairs.append(CArrayConstant('preactivation_%d' % (i+1,), act_shape, const=False, declared=not last_layer))\n",
    "    statements.append(Layer(i, layer.activation)),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "const int preactivation_2_shape[2];\ndouble preactivation_2[4];\n\n"
     ]
    }
   ],
   "source": [
    "print(pairs[-1].actual_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n        mmult(\n            preactivation_1, preactivation_1_shape[0], preactivation_1_shape[1],\n            kernel_1, kernel_1_shape[0], kernel_1_shape[1],\n            xA_1\n            );\n        bias_add(\n            xA_1, xA_1_shape[0], xA_1_shape[1],\n            bias_1,\n            preactivation_2\n        );\n        activate(\n            preactivation_2, \n            preactivation_2_shape[0],\n            preactivation_2_shape[1],\n            2\n        );\n        \n"
     ]
    }
   ],
   "source": [
    "print(statements[-1].code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nvoid MLP(\n    double *preactivation_0, const int batch_size, const int n_inputs,\n    double *preactivation_2, const int n_outputs\n);\n\n"
     ]
    }
   ],
   "source": [
    "main_function_code = CCode('''\n",
    "void MLP(\n",
    "    double *preactivation_0, const int batch_size, const int n_inputs,\n",
    "    double *preactivation_{nlayers}, const int n_outputs\n",
    "    ) {{\n",
    "    const int preactivation_0_shape[2] = {{batch_size, n_inputs}};\n",
    "    const int preactivation_{nlayers}_shape[2] = {{batch_size, n_outputs}};\n",
    "    {statements}\n",
    "}}\n",
    "'''.format(\n",
    "    nlayers=len(model.layers),\n",
    "    statements='\\n'.join([st.code for st in statements])\n",
    "),\n",
    "'''\n",
    "void MLP(\n",
    "    double *preactivation_0, const int batch_size, const int n_inputs,\n",
    "    double *preactivation_{nlayers}, const int n_outputs\n",
    ");\n",
    "'''.format(\n",
    "    nlayers=len(model.layers),\n",
    "    statements='\\n'.join([st.code for st in statements])\n",
    ")\n",
    ")\n",
    "print(main_function_code.header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nvoid MLP(\n    double *preactivation_0, const int batch_size, const int n_inputs,\n    double *preactivation_2, const int n_outputs\n    ) {\n    const int preactivation_0_shape[2] = {batch_size, n_inputs};\n    const int preactivation_2_shape[2] = {batch_size, n_outputs};\n    \n        mmult(\n            preactivation_0, preactivation_0_shape[0], preactivation_0_shape[1],\n            kernel_0, kernel_0_shape[0], kernel_0_shape[1],\n            xA_0\n            );\n        bias_add(\n            xA_0, xA_0_shape[0], xA_0_shape[1],\n            bias_0,\n            preactivation_1\n        );\n        activate(\n            preactivation_1, \n            preactivation_1_shape[0],\n            preactivation_1_shape[1],\n            1\n        );\n        \n\n        mmult(\n            preactivation_1, preactivation_1_shape[0], preactivation_1_shape[1],\n            kernel_1, kernel_1_shape[0], kernel_1_shape[1],\n            xA_1\n            );\n        bias_add(\n            xA_1, xA_1_shape[0], xA_1_shape[1],\n            bias_1,\n            preactivation_2\n        );\n        activate(\n            preactivation_2, \n            preactivation_2_shape[0],\n            preactivation_2_shape[1],\n            2\n        );\n        \n}\n\n"
     ]
    }
   ],
   "source": [
    "print(main_function_code.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input_shape = {2, 2};\ninput = {0, 0, 0, 0};\n\nkernel_0_shape = {2, 2};\nkernel_0 = {0.4177353680133819580078125, -1.1437270641326904296875, -0.963105142116546630859375, -0.332814276218414306640625};\n\nxA_0_shape = {2, 2};\nxA_0 = {0, 0, 0, 0};\n\npreactivation_1_shape = {2, 2};\npreactivation_1 = {0, 0, 0, 0};\n\nkernel_1_shape = {2, 2};\nkernel_1 = {-1.11199855804443359375, -0.616218388080596923828125, -0.35410785675048828125, 1.08596217632293701171875};\n\nxA_1_shape = {2, 2};\nxA_1 = {0, 0, 0, 0};\n\n"
     ]
    }
   ],
   "source": [
    "global_code = '\\n'.join([\n",
    "    c.code for c in pairs if c.header != ''\n",
    "])\n",
    "print(global_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_code = '''\n",
    "#include \"MLP.h\"\n",
    "#include \"mm_utils.h\"\n",
    "\n",
    "{globals}\n",
    "\n",
    "{func}\n",
    "'''.format(globals=global_code, func=main_function_code.code)\n",
    "# print(all_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nconst int input_shape[2];\ndouble input[4];\n\nconst int kernel_0_shape[2];\ndouble kernel_0[4];\n\nconst int xA_0_shape[2];\ndouble xA_0[4];\n\nconst int preactivation_1_shape[2];\ndouble preactivation_1[4];\n\nconst int kernel_1_shape[2];\ndouble kernel_1[4];\n\nconst int xA_1_shape[2];\ndouble xA_1[4];\n\n\n\n\nvoid MLP(\n    double *preactivation_0, const int batch_size, const int n_inputs,\n    double *preactivation_2, const int n_outputs\n);\n\n\n"
     ]
    }
   ],
   "source": [
    "all_headers = '''\n",
    "{globals}\n",
    "\n",
    "{func}\n",
    "'''.format(globals='\\n'.join([c.header for c in pairs]), func=main_function_code.header)\n",
    "# print(all_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}